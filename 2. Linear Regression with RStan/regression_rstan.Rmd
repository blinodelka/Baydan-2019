---
title: "Регрессионная модель в RStan"
author: "Марина Дубова"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 9, message = FALSE)
```


```{r}
library(rstan)
library(ggmcmc)
library(bayesplot)
```

## Генерация данных

Y = intercept + slope * X + epsilon

```{r}
set.seed(100)

sample_size <- 100
intercept <- 5
slope <- -0.8
X <- runif(sample_size, 0, 10)
e <- rnorm(sample_size, 0, 3)
Y <- intercept + slope * X + e
plot(X, Y)

data_1 <- list(X = X, Y = Y, N = sample_size)
```

## Модель 1

Оценка смещения, эффекта и стандартного отклонения шума

```{r, message = TRUE}
model_1 <-
'
data {
  int<lower = 0> N;  // размер выборки
  vector[N] X; // вектор с предиктор
  vector[N] Y; // вектор с целевой переменной
}

parameters {
  real intercept; // смещение
  real slope; // угол наклона регрессионной прямой (эффект предиктора на целевую переменную)
  real<lower = 0> sigma; // шум наблюдений
}

model {
  intercept ~ normal(0, 10); // априорное распределение смещения
  slope ~ normal(0, 1); // априорное распределение угла наклона
  sigma ~ cauchy(0, 3); // априорное распределение шума наблюдения
  for(i in 1:N) {
    Y[i] ~ normal(intercept + slope * X[i], sigma); // функция правдоподобия
  }
}
'
```

```{r, message = TRUE}
fit_1 <- stan(model_code = model_1, data = data_1)
```

```{r}
print(fit_1)

summary(fit, R2 = TRUE)
```

```{r}
posterior_1 <- ggs(fit_1)
ggmcmc(D = posterior_1, file = NULL, plot = 'ggs_histogram')
mean(posterior_1$value < 0 & posterior_1$Parameter == "slope")/mean(posterior_1$Parameter == "slope") # вероятность негативного эффекта

ggmcmc(D = posterior_1, file = NULL, plot = 'ggs_traceplot') 
ggmcmc(D = posterior_1, file = NULL, plot = 'ggs_compare_partial') 
ggmcmc(D = posterior_1, file = NULL, plot = 'ggs_autocorrelation') 
```

Проверка апостериорного распределения: предсказывает ли модель реально полученные данные (Y)?

Адаптация модели для генерации предсказаний Y

```{r}
model_2 <-
'
data {
  int<lower = 0> N;  // размер выборки
  vector[N] X; // вектор с предиктор
  vector[N] Y; // вектор с целевой переменной
}

parameters {
  real intercept; // смещение
  real slope; // угол наклона регрессионной прямой (эффект предиктора на целевую переменную)
  real<lower = 0> sigma; // шум наблюдений
}

model {
  intercept ~ normal(0, 10); // априорное распределение смещения
  slope ~ normal(0, 1); // априорное распределение угла наклона
  sigma ~ cauchy(0, 3); // априорное распределение шума наблюдения
  for(i in 1:N) {
    Y[i] ~ normal(intercept + slope * X[i], sigma); // функция правдоподобия
  }
}

generated quantities {
  vector[N] Y_hat; 
  for(i in 1:N) {
    Y_hat[i] = normal_rng(intercept + slope * X[i], sigma); // генерация предсказаний целевой переменной исходя из оценок параметров
  }
}
'
```

```{r, message = TRUE}
fit_2 <- stan(model_code = model_2, data = data_1)

summary(fit_2, R2 = TRUE)
```

```{r}
Y_hat <- rstan :: extract(fit_2)$Y_hat
ncol(Y_hat)
nrow(Y_hat) 
```

```{r}
choice <- sample(1:4000, 11) # случайно выбираем 11 сгенерированных распределений Y
ppc_hist(data_1$Y, Y_hat[choice, ])
```

Далее: анализ чувствительности результата к априорным распределениям

### Задание: линейная модель с одним предиктором и новыми данными + два предиктора

### категориальные переменные

