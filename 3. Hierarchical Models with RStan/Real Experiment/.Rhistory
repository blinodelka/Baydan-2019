library(rstan)
library(ggmcmc) # для классных диагностических графичков
library(rstan)
library(ggmcmc)
library(bayesplot)
library(bridgesampling)
load("data_for_stan.RData")
# change to 1 - contrastive, 0 - assimilative
# data_for_stan$answer.keys <- 1 - data_for_stan$answer.keys
list_for_stan <- list(nY = nrow(data_for_stan), nS = max(data_for_stan$participant), Subj = data_for_stan$participant, size_diff = data_for_stan$size, num_of_trials = data_for_stan$number_of_fixational_trials, Y = data_for_stan$answer.keys)
fit_2 <- stan("stan_model_1_alternative.stan", data = list_for_stan, iter = 6000, warmup = 2000)
fit_2 <- stan("stan_model_1_alternative.stan", data = list_for_stan, iter = 6000, warmup = 2000)
print(fit_2)
stan_model_alternative <- stan_model("stan_model_1_alternative.stan", model_name = 'alt1')
fit_alt <- sampling(stan_model_alternative, list_for_stan, iter = 20000, warmup = 2000)
fit_alt <- sampling(stan_model_alternative, list_for_stan, iter = 20000, warmup = 2000)
alt_l <- bridge_sampler(fit_alt, silent = TRUE)
print(alt_l)
stan_model_cog <- stan_model("stan_model_1.stan", model_name = 'cog1')
stan_model_cog <- stan_model("stan_model_1.stan", model_name = 'cog1')
fit_cog <- sampling(stan_model_cog, list_for_stan, iter = 20000, warmup = 2000)
cog_l <- bridge_sampler(fit_cog, silent = TRUE)
print(cog_l)
stan_model_cog <- stan_model("stan_model_1.stan", model_name = 'cog1')
fit_cog <- sampling(stan_model_cog, list_for_stan, iter = 20000, warmup = 2000)
stan_model_cog <- stan_model("stan_model_1.stan", model_name = 'cog1')
stan_model_cog <- stan_model("stan_model_1.stan", model_name = 'cog1')
stan_model_cog <- stan_model("stan_model_1.stan", model_name = 'cog1')
fit_1 <- stan("stan_model_1.stan", data = list_for_stan, iter = 6000, warmup = 2000)
print(fit_1)
fit_1 <- stan("stan_model_1.stan", data = list_for_stan, iter = 16000, warmup = 2000)
print(fit_1)
fit_1 <- stan("stan_model_1.stan", data = list_for_stan, iter = 16000, warmup = 4000)
print(fit_1)
fit_1 <- stan("stan_model_1.stan", data = list_for_stan, iter = 16000, warmup = 7000)
print(fit_1)
help("stan")
fit_1 <- stan("stan_model_1.stan", data = list_for_stan, iter = 16000, warmup = 10000)
print(fit_1)
stan_model_cog <- stan_model("stan_model_1.stan", model_name = 'cog1')
fit_cog <- sampling(stan_model_cog, list_for_stan, iter = 30000, warmup = 6000)
cog_l <- bridge_sampler(fit_cog, silent = TRUE)
print(cog_l)
knitr::opts_chunk$set(echo = TRUE, fig.width = 9, message = FALSE)
plot(fit_1)
set.seed(100)
N <- 500
true_mu <- 90
test_scores <- rnorm(N, true_mu, 15)
data_1 <- list(N = length(test_scores), X = test_scores) # формат данных для stan - список с переменными
model_1 <-
'
data {
int<lower = 0> N;  // размер выборки (больше 0)
vector[N] X; // наблюдения - вектор длины N
}
parameters {
real<lower = 0> mu; // среднее нормального распределения, которое необходимо оценить
}
model {
mu ~ normal(100, 25); // априорное распределение среднего
X ~ normal(mu, 15); // функция правдоподобия
}
'
fit_1 <- stan(model_code = model_1,
data = data_1,
chains = 3,
iter = 5000,
warmup = 1000)
# ! необходимо использовать несколько цепей для диагностики результата
print(fit_1)
plot(fit_1)
posterior(fit_2)
posterior_fit_2
posterior_fit_1
library(rstan)
library(ggmcmc) # для классных диагностических графичков
set.seed(100)
N <- 500
true_mu <- 90
test_scores <- rnorm(N, true_mu, 15)
data_1 <- list(N = length(test_scores), X = test_scores) # формат данных для stan - список с переменными
model_1 <-
'
data {
int<lower = 0> N;  // размер выборки (больше 0)
vector[N] X; // наблюдения - вектор длины N
}
parameters {
real<lower = 0> mu; // среднее нормального распределения, которое необходимо оценить
}
model {
mu ~ normal(100, 25); // априорное распределение среднего
X ~ normal(mu, 15); // функция правдоподобия
}
'
fit_1 <- stan(model_code = model_1, # код модели
data = data_1, # список с данными для модели
chains = 3, # количество цепей (со случайными изначальными точками), из которых генерируются апостериорные распределения
iter = 5000, # количество сэмплирований из цепей
warmup = 1000) # количество сэмплирований, после которого мы начинаем записывать результат
posterior_mu <- ggs(fit_1) # считываем сэмплы в удобном для ggmcmc формате
ggmcmc(D = posterior_mu, file = NULL, plot = 'ggs_histogram') # апостериорное распределение параметра
mean(posterior_mu$value > 90) # какую часть апостериорного распределения составляют средние больше 90?
ggmcmc(D = posterior_mu, file = NULL, plot = 'ggs_traceplot') # должен выглядить как белый шум, все цепи варьируют вокруг одинаковых значений. Если нет - можно попробовать увеличить warmup
print(posterior_fit_1)
posterior_mu
help("ggmcmc")
model_2 <-
'
data {
int<lower = 0> N;  // размер выборки (больше 0)
vector[N] X; // наблюдения - вектор длины N
}
parameters {
real<lower = 0> mu; // среднее нормального распределения, которое необходимо оценить
real<lower = 0> sigma; // стандартное отклонение нормального распределения, которое необходимо оценить
}
model {
mu ~ normal(100, 25); // априорное распределение среднего
sigma ~ cauchy(0,3); // априорное распределение стандартного отклонения
X ~ normal(mu, sigma); // функция правдоподобия
}
'
fit_2 <- stan(model_code = model_2, data = data_1, chains = 3, iter = 5000, warmup = 1000)
print(fit_2)
posterior_fit_2 <- ggs(fit_2)
mean(posterior_fit_2$value > 90 & posterior_fit_2$Parameter == "mu")/mean(posterior_fit_2$Parameter == "mu")
ggmcmc(D = posterior_fit_2, file = NULL, plot = 'ggs_histogram')
ggs_pairs(posterior_fit_2, lower = list(continuous = "density"))
ggs_pairs(posterior_fit_2, lower = list(continuous = "density"))
ggs_pairs(posterior_fit_2, lower = list(continuous = "density"))
ggs_pairs(posterior_fit_2, lower = list(continuous = "density"))
ggmcmc(D = posterior_fit_2, file = NULL, plot = 'ggs_histogram')
help("ggs_pairs")
ggs_pairs(posterior_fit_2, lower = list(continuous = "points"))
ggs_pairs(posterior_fit_2, lower = list(continuous = "density"))
set.seed(100)
sample_size <- 100
intercept <- 5
slope <- -0.8
X <- runif(sample_size, 0, 10)
e <- rnorm(sample_size, 0, 3)
Y <- intercept + slope * X + e
plot(X, Y)
data_1 <- list(X = X, Y = Y, N = sample_size)
model_2 <-
'
data {
int<lower = 0> N;  // размер выборки
vector[N] X; // вектор с предиктор
vector[N] Y; // вектор с целевой переменной
}
parameters {
real intercept; // смещение
real slope; // угол наклона регрессионной прямой (эффект предиктора на целевую переменную)
real<lower = 0> sigma; // шум наблюдений
}
model {
intercept ~ normal(0, 10); // априорное распределение смещения
slope ~ normal(0, 1); // априорное распределение угла наклона
sigma ~ cauchy(0, 3); // априорное распределение шума наблюдения
for(i in 1:N) {
Y[i] ~ normal(intercept + slope * X[i], sigma); // функция правдоподобия
}
}
generated quantities {
vector[N] Y_hat;
for(i in 1:N) {
Y_hat[i] = normal_rng(intercept + slope * X[i], sigma); // генерация предсказаний целевой переменной исходя из оценок параметров
}
}
'
fit_2 <- stan(model_code = model_2, data = data_1)
Y_hat <- rstan :: extract(fit_2)$Y_hat
ncol(Y_hat)
nrow(Y_hat)
choice <- sample(1:4000, 11) # случайно выбираем 11 сгенерированных распределений Y
ppc_hist(data_1$Y, Y_hat[choice, ])
library(rstan)
library(ggmcmc)
library(bayesplot)
library(bridgesampling)
load("data_for_stan.RData")
getwd()
setwd("/home/marina/Documents/Teaching/Baydan 2019/Baydan_2019/3. Hierarchical Models with RStan/Real Experiment")
load("data_for_stan.RData")
setwd("/home/marina/Documents/Teaching/Baydan 2019/Baydan_2019/3. Hierarchical Models with RStan/Real Experiment")
load("data_for_stan.RData")
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(rstan)
library(ggmcmc)
library(bayesplot)
library(bridgesampling)
load("data_for_stan.RData")
list_for_stan <- list(nY = nrow(data_for_stan), nS = max(data_for_stan$participant), Subj = data_for_stan$participant, size_diff = data_for_stan$size, num_of_trials = data_for_stan$number_of_fixational_trials, Y = data_for_stan$answer.keys)
# компилируем модели, соответствующие двум гипотезам
stanmodelH0 <- stan_model('H0_1.stan', model_name = 'H0')
stanmodelH1 <- stan_model('H1_1.stan', model_name = 'H1')
# сэмплируем "предсказания" каждой из гипотез (моделей), нужно сгенерировать очень много
fit_H0 <- sampling(stanmodelH0, list_for_stan, iter = 20000, warmup = 1000)
fit_H1 <- sampling(stanmodelH1, list_for_stan, iter = 20000, warmup = 1000)
print(fit_H0)
print(fit_H1)
posterior_H0 <- ggs(fit_H0)
posterior_H1 <- ggs(fit_H1)
ggmcmc(D = posterior_H0, file = NULL, family = 'beta_mu', plot = 'ggs_histogram')
posterior_H0 <- ggs(fit_H0)
posterior_H1 <- ggs(fit_H1)
ggmcmc(D = posterior_H0, file = NULL, family = 'beta', plot = 'ggs_histogram')
ggmcmc(D = posterior_H1, file = NULL, family = 'beta', plot = 'ggs_histogram')
ggmcmc(D = posterior_H0, file = NULL, family = 'beta', plot = 'ggs_compare_partial')
