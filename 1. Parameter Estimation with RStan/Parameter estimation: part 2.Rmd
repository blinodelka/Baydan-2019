---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 9, message = FALSE)
```

```{r}
library(rstan)
library(ggmcmc)
```

## Генерация данных

Биномиальное распределение: результаты теста с n аналогичными вопросами для какого-либо испытуемого

```{r}
set.seed(100)

N <- 100
true_p <- 0.65
N_test_questions <- 40
test_scores <- rbinom(N, N_test_questions, true_p)
data_1 <- list(N = length(test_scores), N_q = N_test_questions, X = test_scores) # формат данных для stan - список с переменными
```


```{r}
model_1 <-
'
data {
  int<lower = 0> N;  // размер выборки (больше 0)
  int<lower = 0> N_q; // количество вопросов теста
  int<lower = 0> X[N]; // наблюдения - вектор длины N
}

parameters {
  real<lower = 0, upper = 1> p; // вероятность единичного успеха в биномиальном распределении
}

model {
  p ~ uniform(0.5, 1); // априорное распределение вероятности успеха в тесте
  for (i in 1:N){
    X[i] ~ binomial(N_q, p); // функция правдоподобия
  }
}
'
```


```{r, message = TRUE}
fit_1 <- stan(model_code = model_1, # код модели
              data = data_1, # список с данными для модели
              chains = 3, # количество цепей (со случайными изначальными точками), из которых генерируются апостериорные распределения
              iter = 5000, # количество сэмплирований из цепей
              warmup = 1000) # количество сэмплирований, после которого мы начинаем записывать результат
```

```{r}
print(fit_1)
```

```{r}
plot(fit_1)
```


```{r}
posterior_p <- ggs(fit_1) # считываем сэмплы в удобном для ggmcmc формате
ggmcmc(D = posterior_p, file = NULL, plot = 'ggs_histogram') # апостериорное распределение параметра
mean(posterior_p$value > 0.7) # какую часть апостериорного распределения составляют вероятности больше 0.7
```

```{r}
ggmcmc(D = posterior_p, file = NULL, plot = 'ggs_traceplot') # должен выглядить как белый шум, все цепи варьируют вокруг одинаковых значений. Если нет - можно попробовать увеличить warmup
```


```{r}
ggmcmc(D = posterior_p, file = NULL, plot = 'ggs_compare_partial') # полное и частичное апостериорное распределение в идеале должны полностью перекрывать друг друга 
```

```{r}
ggmcmc(D = posterior_p, file = NULL, plot = 'ggs_autocorrelation') # в идеале: все значения после 1 на нуле или очень близки к нулю
```

## Информативное априорное распределение на p

```{r}
model_2 <-
'
data {
  int<lower = 0> N;  // размер выборки (больше 0)
  int<lower = 0> N_q; // количество вопросов теста
  int<lower = 0> X[N]; // наблюдения - вектор длины N
}

parameters {
  real<lower = 0, upper = 1> p; // вероятность единичного успеха в биномиальном распределении
}

model {
  p ~ normal(0.9, 0.05); // априорное распределение вероятности успеха в тесте
  for (i in 1:N){
    X[i] ~ binomial(N_q, p); // функция правдоподобия
  }
}
'
```

```{r, message = TRUE}
fit_1_inf <- stan(model_code = model_2, # код модели
              data = data_1, # список с данными для модели
              chains = 3, # количество цепей (со случайными изначальными точками), из которых генерируются апостериорные распределения
              iter = 5000, # количество сэмплирований из цепей
              warmup = 1000) # количество сэмплирований, после которого мы начинаем записывать результат
```

```{r}
print(fit_1_inf)
plot(fit_1_inf)
```

```{r}
posterior_p_1_inf <- ggs(fit_1_inf) # считываем сэмплы в удобном для ggmcmc формате
ggs_histogram(posterior_p_1_inf) + xlim(0.6, 0.8) # апостериорное распределение параметра с информативным априорным распределением
ggs_histogram(posterior_p) + xlim(0.6, 0.8) # апостериорное распределение параметра

mean(posterior_p_1_inf$value > 0.7) # какую часть апостериорного распределения составляют вероятности больше 0.7
```

```{r}
ggmcmc(D = posterior_p_1_inf, file = NULL, plot = 'ggs_traceplot') # должен выглядить как белый шум, все цепи варьируют вокруг одинаковых значений. Если нет - можно попробовать увеличить warmup
```

```{r}
ggmcmc(D = posterior_p_1_inf, file = NULL, plot = 'ggs_compare_partial') # полное и частичное апостериорное распределение в идеале должны полностью перекрывать друг друга 
```

```{r}
ggmcmc(D = posterior_p_1_inf, file = NULL, plot = 'ggs_autocorrelation') # полное и частичное апостериорное распределение в идеале должны полностью перекрывать друг друга 
```

## Мало данных

```{r}
set.seed(100)

N <- 10
true_p <- 0.65
N_test_questions <- 30
test_scores <- rbinom(N, N_test_questions, true_p)
data_2 <- list(N = length(test_scores), N_q = N_test_questions, X = test_scores) # формат данных для stan - список с переменными
```


```{r, message = TRUE}
fit_2 <- stan(model_code = model_1, # код модели
              data = data_2, # список с данными для модели
              chains = 3, # количество цепей (со случайными изначальными точками), из которых генерируются апостериорные распределения
              iter = 5000, # количество сэмплирований из цепей
              warmup = 1000) # количество сэмплирований, после которого мы начинаем записывать результат
```

```{r}
print(fit_2)
plot(fit_2)
```

```{r}
posterior_p_2 <- ggs(fit_2) # считываем сэмплы в удобном для ggmcmc формате
ggmcmc(D = posterior_p_2, file = NULL, plot = 'ggs_histogram') # апостериорное распределение параметра
mean(posterior_p_2$value > 0.7) # какую часть апостериорного распределения составляют вероятности больше 0.7
```

```{r}
ggmcmc(D = posterior_p_2, file = NULL, plot = 'ggs_traceplot') # должен выглядить как белый шум, все цепи варьируют вокруг одинаковых значений. Если нет - можно попробовать увеличить warmup
```

```{r}
ggmcmc(D = posterior_p_2, file = NULL, plot = 'ggs_compare_partial') # полное и частичное апостериорное распределение в идеале должны полностью перекрывать друг друга 
```

```{r}
ggmcmc(D = posterior_p_2, file = NULL, plot = 'ggs_autocorrelation') # полное и частичное апостериорное распределение в идеале должны полностью перекрывать друг друга 
```


Информативное априорное распределение и мало данных

```{r, message = TRUE}
fit_2_inf <- stan(model_code = model_2, # код модели с информативным априорным распределением
              data = data_2, # список с данными для модели
              chains = 3, # количество цепей (со случайными изначальными точками), из которых генерируются апостериорные распределения
              iter = 5000, # количество сэмплирований из цепей
              warmup = 1000) # количество сэмплирований, после которого мы начинаем записывать результат
```

```{r}
print(fit_2_inf)
print(fit_2)
plot(fit_2_inf)
plot(fit_2)
```

```{r}
posterior_p_2_inf <- ggs(fit_2_inf) # считываем сэмплы в удобном для ggmcmc формате
ggs_histogram(posterior_p_2_inf) + xlim(0.6, 0.8) # апостериорное распределение параметра c информативным априорным распределением
ggs_histogram(posterior_p_2) + xlim(0.6, 0.8) # апостериорное распределение параметра без информативного априорного распределения
mean(posterior_p_2_inf$value > 0.7) # какую часть апостериорного распределения составляют вероятности больше 0.7
```

```{r}
ggmcmc(D = posterior_p_2_inf, file = NULL, plot = 'ggs_traceplot') # должен выглядить как белый шум, все цепи варьируют вокруг одинаковых значений. Если нет - можно попробовать увеличить warmup
```

```{r}
ggmcmc(D = posterior_p_2_inf, file = NULL, plot = 'ggs_compare_partial') # полное и частичное апостериорное распределение в идеале должны полностью перекрывать друг друга 
```

```{r}
ggmcmc(D = posterior_p_2_inf, file = NULL, plot = 'ggs_autocorrelation') # полное и частичное апостериорное распределение в идеале должны полностью перекрывать друг друга 
```
